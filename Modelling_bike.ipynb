{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "The goal is too find the best model for the prediction, to do this we will start by importing the different data sets, preprocessing them and merging them together. Afterwards will do some feature engineering and some encoding before using auto-sklearn to find the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from flaml import AutoML\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"data/train.parquet\")\n",
    "\n",
    "school_hols_df = pd.read_csv(\"external_data/holidays.csv\")\n",
    "\n",
    "lockdown_periods = [\n",
    "    ('2020-03-18', '2020-05-10'),\n",
    "    ('2020-10-31', '2020-12-14'),\n",
    "    ('2021-04-04', '2021-05-02')\n",
    "]\n",
    "\n",
    "weather_df = pd.read_csv(\n",
    "    \"external_data/H_75_previous-2020-2022.csv.gz\",\n",
    "    parse_dates=[\"AAAAMMJJHH\"],\n",
    "    date_format=\"%Y%m%d%H\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\";\",\n",
    ").rename(columns={\"AAAAMMJJHH\": \"date\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features you want to keep\n",
    "selected_features = ['NUM_POSTE', 'date', 'RR1', 'DRR1', 'FF', 'T', 'TCHAUSSEE', 'U', 'GLO']\n",
    "\n",
    "# Subset the DataFrame to only these columns\n",
    "weather_df = weather_df[selected_features]\n",
    "\n",
    "# Keep only rows where \"NUM_POSTE\" is equal to 75114001 and then drop the column\n",
    "weather_df = weather_df[weather_df['NUM_POSTE'] == 75114001]\n",
    "weather_df.drop('NUM_POSTE', axis=1, inplace=True)\n",
    "\n",
    "#Interpolate missing values\n",
    "weather_df.set_index('date', inplace=True)\n",
    "weather_df = weather_df.interpolate(method='time')\n",
    "weather_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge function\n",
    "\n",
    "def prepare_and_merge_data(train_df, weather_df, school_hols_df):\n",
    "    # Convert date columns to datetime\n",
    "    train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "    school_hols_df['date'] = pd.to_datetime(school_hols_df['date'])\n",
    "\n",
    "    # Convert boolean vacations to int (True -> 1, False -> 0)\n",
    "    school_hols_df['vacances_zone_c'] = school_hols_df['vacances_zone_c'].astype(int)\n",
    "\n",
    "    # Add bank holidays\n",
    "    fr_holidays = holidays.France()\n",
    "    train_df['is_bank_holiday'] = train_df['date'].dt.date.apply(\n",
    "        lambda d: 1 if d in fr_holidays else 0\n",
    "    )\n",
    "\n",
    "    # Define lockdown periods\n",
    "    lockdown_periods = [\n",
    "        ('2020-03-18', '2020-05-10'),\n",
    "        ('2020-10-31', '2020-12-14'),\n",
    "        ('2021-04-04', '2021-05-02')\n",
    "    ]\n",
    "\n",
    "    def in_lockdown(dt):\n",
    "        d_str = dt.strftime('%Y-%m-%d')\n",
    "        return 1 if any(start <= d_str <= end for start, end in lockdown_periods) else 0\n",
    "\n",
    "    train_df['is_lockdown'] = train_df['date'].apply(in_lockdown)\n",
    "\n",
    "    # Merge school holidays (daily data) into the hourly train data\n",
    "    train_df['date_only'] = train_df['date'].dt.floor('D')\n",
    "    train_df = train_df.merge(\n",
    "        school_hols_df[['date', 'vacances_zone_c']],\n",
    "        left_on='date_only',\n",
    "        right_on='date',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    train_df.rename(columns={'vacances_zone_c': 'school_holidays'}, inplace=True)\n",
    "\n",
    "    # Clean up after merge\n",
    "    train_df.drop(columns=['date_only', 'date_y'], inplace=True)\n",
    "    train_df.rename(columns={'date_x': 'date'}, inplace=True)\n",
    "\n",
    "    # Merge weather data\n",
    "    merged_df = pd.merge(train_df, weather_df, on='date', how='left')\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = prepare_and_merge_data(train_df, weather_df, school_hols_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode date features\n",
    "\n",
    "def encode_time_features(df, date_col='date'):\n",
    "    df['year'] = df[date_col].dt.year\n",
    "    df['quarter'] = df[date_col].dt.quarter\n",
    "    df['month'] = df[date_col].dt.month\n",
    "    df['day'] = df[date_col].dt.day\n",
    "    df['weekday'] = df[date_col].dt.weekday\n",
    "    df['hour'] = df[date_col].dt.hour\n",
    "\n",
    "    # Cyclical encodings\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "    # Weekend indicator\n",
    "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
    "\n",
    "    return df.drop(columns=[date_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = encode_time_features(final_train_df, date_col='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance feature from center of Paris coordinates\n",
    "ref_lat, ref_lon = 48.8566, 2.3522\n",
    "final_train_df['dist_center'] = np.sqrt((final_train_df['latitude'] - ref_lat)**2 + (final_train_df['longitude'] - ref_lon)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop reredundant columns (except counter_id and site_id for encoding)\n",
    "cols_to_drop = ['counter_name', 'counter_technical_id', 'site_name', 'latitude', 'longitude', 'coordinates', 'bike_count', 'counter_installation_date']\n",
    "final_train_df.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode counter_id and site_id\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Fit and transform on counter_id\n",
    "counter_id_encoded = encoder.fit_transform(final_train_df[['counter_id']])\n",
    "counter_id_encoded_df = pd.DataFrame(counter_id_encoded, columns=encoder.get_feature_names_out(['counter_id']))\n",
    "final_train_df = pd.concat([final_train_df, counter_id_encoded_df], axis=1)\n",
    "\n",
    "# Fit and transform on site_id\n",
    "site_id_encoded = encoder.fit_transform(final_train_df[['site_id']])\n",
    "site_id_encoded_df = pd.DataFrame(site_id_encoded, columns=encoder.get_feature_names_out(['site_id']))\n",
    "final_train_df = pd.concat([final_train_df, site_id_encoded_df], axis=1)\n",
    "\n",
    "\n",
    "# Drop the original counter_id and site_id after encoding\n",
    "final_train_df.drop(columns=['counter_id', 'site_id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "y = final_train_df['log_bike_count']\n",
    "X = final_train_df.drop(columns=['log_bike_count'])\n",
    "\n",
    "# Initialize FLAML AutoML\n",
    "automl = AutoML(estimator_list=[\"rf\", \"xgboost\", \"lgbm\", \"catboost\"])\n",
    "\n",
    "# Fit AutoML model using Cross-Validation\n",
    "automl.fit(\n",
    "    X,\n",
    "    y,\n",
    "    task=\"regression\",\n",
    "    time_budget=1500,\n",
    "    eval_method=\"cv\",\n",
    "    metric=\"rmse\",\n",
    "    n_splits=3,\n",
    "    verbose=2,\n",
    "    split_type=TimeSeriesSplit(),\n",
    "    estimator_list = [\n",
    "        \"extra_tree\",\n",
    "        \"histgb\",\n",
    "        \"lgbm\",\n",
    "        \"rf\",\n",
    "        \"xgboost\",\n",
    "        \"xgb_limitdepth\",\n",
    "    ],\n",
    "    force_cancel=True,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: extra_tree\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Model:\", automl.best_estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
